{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a86a3b-183f-4e67-976f-4358dc6ae2ae",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-12-07T08:47:57.3656311Z",
       "execution_start_time": "2025-12-07T08:47:47.7243572Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c79f75ab-3f3e-4381-9873-1dd62467b960",
       "queued_time": "2025-12-07T08:47:47.7232156Z",
       "session_id": "cfc06aad-a3f6-4fae-9237-ec36ca71eefe",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 28,
       "statement_ids": [
        28
       ]
      },
      "text/plain": [
       "StatementMeta(, cfc06aad-a3f6-4fae-9237-ec36ca71eefe, 28, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw files...\n",
      "Table 'silver_news' exists. Performing Incremental Merge...\n",
      "Merge Complete.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BRONZE_PATH = \"Files/Bronze/Landing\"\n",
    "SILVER_TABLE_NAME = \"silver_news\"\n",
    "\n",
    "try:\n",
    "    # 1. READ RAW DATA\n",
    "    df_raw = spark.read.json(BRONZE_PATH)\n",
    "    \n",
    "    # 2. CLEANING\n",
    "    df_clean = df_raw.withColumn(\"date\", to_timestamp(col(\"date\"))) \\\n",
    "                     .withColumn(\"processed_time\", current_timestamp()) \\\n",
    "                     .select(\n",
    "                         col(\"title\"),\n",
    "                         col(\"url\"),\n",
    "                         col(\"snippet\"),  # âœ… FIXED: Use \"snippet\" not \"body\"\n",
    "                         col(\"source\"),\n",
    "                         col(\"date\"),\n",
    "                         col(\"competitor_tag\"),\n",
    "                         col(\"processed_time\")\n",
    "                     )\n",
    "    \n",
    "    # Drop null URLs\n",
    "    df_clean = df_clean.filter(col(\"url\").isNotNull())\n",
    "    \n",
    "    # 3. DEDUPLICATION\n",
    "    if spark.catalog.tableExists(SILVER_TABLE_NAME):\n",
    "        # Incremental Merge\n",
    "        delta_table = DeltaTable.forName(spark, SILVER_TABLE_NAME)\n",
    "        \n",
    "        delta_table.alias(\"target\").merge(\n",
    "            df_clean.alias(\"source\"),\n",
    "            \"target.url = source.url\"\n",
    "        ).whenNotMatchedInsertAll().execute()\n",
    "        \n",
    "        message = \"Merge Complete - Incremental Load\"\n",
    "    else:\n",
    "        # First time creation\n",
    "        df_clean.write.format(\"delta\").saveAsTable(SILVER_TABLE_NAME)\n",
    "        message = \"Table Created - Initial Load\"\n",
    "    \n",
    "    # Prepare pipeline output\n",
    "    record_count = df_clean.count()\n",
    "    result = {\n",
    "        \"status\": \"success\",\n",
    "        \"message\": message,\n",
    "        \"records_processed\": record_count,\n",
    "        \"table_name\": SILVER_TABLE_NAME,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Exit with JSON for pipeline\n",
    "    mssparkutils.notebook.exit(json.dumps(result))\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle errors gracefully\n",
    "    error_result = {\n",
    "        \"status\": \"failed\",\n",
    "        \"error\": str(e),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    mssparkutils.notebook.exit(json.dumps(error_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420ed92-cbf5-4cc9-b5cb-521b4d35868e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "44aa9b25-2bd6-4716-80b7-1a3f6d81a778",
    "default_lakehouse_name": "Lh_Market_Pulse",
    "default_lakehouse_workspace_id": "230d9539-aa80-4ea1-847e-f9acc0f18dd3",
    "known_lakehouses": [
     {
      "id": "44aa9b25-2bd6-4716-80b7-1a3f6d81a778"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
